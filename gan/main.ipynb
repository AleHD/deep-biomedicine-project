{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jurri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\jurri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"..\")\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import torch, gc\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "from src.dataset import ImageDataset\n",
    "from src.utils import get_indices\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from unet.model import UNet\n",
    "from unet.training import Trainer\n",
    "import unet.utils\n",
    "\n",
    "from gan.discriminator import Discriminator\n",
    "from mwcnn.mwcnn import MWCNN\n",
    "\n",
    "from src.loss_functions import FFTloss, VGGPerceptualLoss\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Dataset part used for testing\n",
    "TEST_SPLIT = 0.15\n",
    "# Batch size for training. Limited by GPU memory\n",
    "BATCH_SIZE = 5\n",
    "# Dataset folder used\n",
    "DATASET_USED = 'e9_5_GLM87a_cycle1_8_8'\n",
    "#DATASET_USED = 'e12_5_slide7_round1_section1'\n",
    "# Full Dataset path\n",
    "DATASETS = ['e9_5_GLM87a_cycle1_8_8', 'e12_5_slide7_round1_section1' , 'train', 'val']\n",
    "ROOTDIR = '../data/mip2edof_2samples/'\n",
    "\n",
    "# Training Epochs\n",
    "EPOCHS = 200\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jurri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "image_dataset = ImageDataset(ROOTDIR, DATASETS, normalize=\"percentile\")\n",
    "\n",
    "\n",
    "train_indices,validation_indices, test_indices = get_indices(len(image_dataset), image_dataset.root_dir, TEST_SPLIT, new=True)\n",
    "train_sampler,validation_sampler, test_sampler = SubsetRandomSampler(train_indices),SubsetRandomSampler(validation_indices), SubsetRandomSampler(test_indices)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(image_dataset, BATCH_SIZE, sampler=train_sampler)\n",
    "validationloader = torch.utils.data.DataLoader(image_dataset, BATCH_SIZE, sampler=validation_sampler)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(image_dataset, 1, sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain UNET\n",
    "First pretrain a UNET and MWCNN, afterwards, this can be used in the GAN set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_loss(num_epochs,train_losses):\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "    plt.title('Training Loss Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "filter_num = [16,32,64,128,256]\n",
    "unet_model = UNet(filter_num).to(device)\n",
    "# Training\n",
    "unet_trainer = Trainer(unet_model,device)\n",
    "unet_loss_record, validation_loss_record = unet_trainer.train(EPOCHS,trainloader,validationloader,mini_batch=BATCH_SIZE)\n",
    "\n",
    "MODEL_NAME = f\"models/UNet-{filter_num}.pth\"\n",
    "torch.save(unet_model, MODEL_NAME)\n",
    "\n",
    "print(f'Training finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(EPOCHS,unet_loss_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "criterion = torch.nn.L1Loss()\n",
    "MWCNN_model = MWCNN(n_feats=64, n_colors=1, batch_normalize=False).to(device)\n",
    "\n",
    "# Training\n",
    "MWCNN_trainer = Trainer(MWCNN_model, criterion, device, clear_cache=True)\n",
    "mwcnn_loss_record = MWCNN_trainer.train(EPOCHS,trainloader,mini_batch=1)\n",
    "torch.save(unet_model, MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(EPOCHS,mwcnn_loss_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make gan and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jurri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jurri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 1024, 1024])\n",
      "torch.Size([5, 32, 1024, 1024])\n",
      "torch.Size([5, 32, 512, 512])\n",
      "torch.Size([5, 128, 128, 128])\n",
      "torch.Size([5, 1, 1024, 1024])\n",
      "torch.Size([5, 32, 1024, 1024])\n",
      "torch.Size([5, 32, 512, 512])\n",
      "torch.Size([5, 128, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "generator = unet_model\n",
    "discriminator = Discriminator(n_feats=64, patch_size=1024)\n",
    "discriminator = discriminator.to(device)\n",
    "discriminator.train()\n",
    "\n",
    "d_optim = optim.Adam(discriminator.parameters(), lr = 1e-4)\n",
    "g_optim = optim.Adam(generator.parameters(), lr = 1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(g_optim, step_size = 2000, gamma = 0.1)\n",
    "\n",
    "VGG_loss = VGGPerceptualLoss(resize=False).to(device)\n",
    "cross_ent = torch.nn.BCELoss()\n",
    "L1_loss = torch.nn.L1Loss()\n",
    "real_label = torch.ones((BATCH_SIZE, 1)).to(device)\n",
    "fake_label = torch.zeros((BATCH_SIZE, 1)).to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    for i, data in enumerate(trainloader):\n",
    "        # Loading data to device used.\n",
    "        noisy = data['input_image'].to(device)\n",
    "        sharp = data['output_image'].float().to(device)\n",
    "                    \n",
    "        ## Training Discriminator\n",
    "        output = generator(noisy)\n",
    "        fake_prob = discriminator(output)\n",
    "        real_prob = discriminator(sharp)\n",
    "        \n",
    "        d_loss_real = cross_ent(real_prob, real_label)\n",
    "        d_loss_fake = cross_ent(fake_prob, fake_label)\n",
    "        \n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        g_optim.zero_grad()\n",
    "        d_optim.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optim.step()\n",
    "        \n",
    "        ## Training Generator\n",
    "        output, _ = generator(noisy)\n",
    "        fake_prob = discriminator(output)\n",
    "        \n",
    "        _percep_loss, hr_feat, sr_feat = VGG_loss((sharp + 1.0) / 2.0, (output + 1.0) / 2.0)\n",
    "        \n",
    "        L1_loss = L1_loss(output, sharp)\n",
    "        percep_loss = _percep_loss\n",
    "        adversarial_loss = cross_ent(fake_prob, real_label)\n",
    "        \n",
    "        g_loss = percep_loss + adversarial_loss + L1_loss\n",
    "        \n",
    "        g_optim.zero_grad()\n",
    "        d_optim.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optim.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        \n",
    "    if epoch % 2 == 0:\n",
    "        print(epoch)\n",
    "        print(g_loss.item())\n",
    "        print(d_loss.item())\n",
    "        print('=========')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show results\n",
    "generator.eval()\n",
    "\n",
    "for data in testloader:\n",
    "    noisy = data['input_image'].to(device)\n",
    "    # sharp = data['output_image'].to(device)\n",
    "    output = generator(noisy)\n",
    "    output = output.cpu().numpy()\n",
    "    output = (output + 1.0) / 2.0\n",
    "    output = output.transpose(1,2,0)\n",
    "    result = Image.fromarray((output * 255.0).astype(np.uint8))\n",
    "    result.save('./result/res_%04d.png'%i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
