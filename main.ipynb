{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jurri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\jurri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import SubsetRandomSampler, ConcatDataset\n",
    "\n",
    "from src.dataset import ImageDataset\n",
    "from src.utils import get_indices\n",
    "from src.model import MWCNN\n",
    "from src.layers import DWT, IWT, FFTloss\n",
    "from src.training import Trainer\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Dataset part used for testing\n",
    "TEST_SPLIT = 0.2\n",
    "# Batch size for training. Limited by GPU memory\n",
    "BATCH_SIZE = 6\n",
    "# Dataset folder used\n",
    "DATASETS = ['e9_5_GLM87a_cycle1_8_8', 'e12_5_slide7_round1_section1']\n",
    "ROOTDIR = 'data\\mip2edof_2samples'\n",
    "# Training Epochs\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Initiate train and test loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jurri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "image_dataset = ImageDataset(ROOTDIR, DATASETS)\n",
    "\n",
    "train_indices, test_indices = get_indices(len(image_dataset), image_dataset.root_dir, TEST_SPLIT)\n",
    "train_sampler, test_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(test_indices)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(image_dataset, BATCH_SIZE, sampler=train_sampler)\n",
    "testloader = torch.utils.data.DataLoader(image_dataset, 1, sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Show example of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# fig, ax = plt.subplots(nrows=6, ncols=2, figsize=(20, 60))\n",
    "# for data in trainloader:\n",
    "#     input_image = data['input_image'].squeeze().permute(1, 2, 0)\n",
    "#     output_image = data['output_image'].squeeze().permute(1, 2, 0)\n",
    "#     for i in range(input_image.shape[2]):\n",
    "#         ax[i,0].imshow(input_image[:,:,i], cmap='gray', vmin=0, vmax=16383, aspect='equal')\n",
    "#         ax[i,1].imshow(output_image[:,:,i], cmap='gray', vmin=0, vmax=16383, aspect='equal')\n",
    "#     break\n",
    "\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Initiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwt = DWT()\n",
    "iwt = IWT()\n",
    "\n",
    "MWCNN_model = MWCNN(16, dwt, iwt).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Process\n",
      "Batch: 05,\tBatch Loss: 103.6150218\n",
      "Batch: 10,\tBatch Loss: 111.1386963\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jurri\\Documents\\University\\Life sciences and engineering\\Deep learning in biomedicine\\project\\deep-biomedicine-project\\main.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jurri/Documents/University/Life%20sciences%20and%20engineering/Deep%20learning%20in%20biomedicine/project/deep-biomedicine-project/main.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m criterion \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mL1Loss()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jurri/Documents/University/Life%20sciences%20and%20engineering/Deep%20learning%20in%20biomedicine/project/deep-biomedicine-project/main.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m MWCNN_trainer \u001b[39m=\u001b[39m Trainer(MWCNN_model, criterion, device)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jurri/Documents/University/Life%20sciences%20and%20engineering/Deep%20learning%20in%20biomedicine/project/deep-biomedicine-project/main.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m loss_record \u001b[39m=\u001b[39m MWCNN_trainer\u001b[39m.\u001b[39;49mtrain(EPOCHS,trainloader,mini_batch\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jurri/Documents/University/Life%20sciences%20and%20engineering/Deep%20learning%20in%20biomedicine/project/deep-biomedicine-project/main.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining finished!\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jurri\\Documents\\University\\Life sciences and engineering\\Deep learning in biomedicine\\project\\deep-biomedicine-project\\src\\training.py:55\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, epochs, trainloader, mini_batch, learning_rate)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39m# Epoch Loop\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m     53\u001b[0m \n\u001b[0;32m     54\u001b[0m     \u001b[39m# Training a single epoch\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m     epoch_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_epoch(trainloader, mini_batch)\n\u001b[0;32m     57\u001b[0m     \u001b[39m# Collecting all epoch loss values for future visualization.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     loss_record\u001b[39m.\u001b[39mappend(epoch_loss)\n",
      "File \u001b[1;32mc:\\Users\\jurri\\Documents\\University\\Life sciences and engineering\\Deep learning in biomedicine\\project\\deep-biomedicine-project\\src\\training.py:95\u001b[0m, in \u001b[0;36mTrainer._train_epoch\u001b[1;34m(self, trainloader, mini_batch)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     94\u001b[0m \u001b[39m# Calculation predicted output using forward pass.\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(image)\n\u001b[0;32m     97\u001b[0m \u001b[39m# Calculating the loss value.\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39m# Hint: self.criterion\u001b[39;00m\n\u001b[0;32m     99\u001b[0m loss_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(output, mask)\n",
      "File \u001b[1;32mc:\\Users\\jurri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jurri\\Documents\\University\\Life sciences and engineering\\Deep learning in biomedicine\\project\\deep-biomedicine-project\\src\\model.py:106\u001b[0m, in \u001b[0;36mMWCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" \u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39mForward propagation of the network.\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[39m# Encoding Part of Network.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[39m# Hint: Do not forget to add activate function, e.g. ReLU, between to convolution layers. (same for the bottlenect and decoder)\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \n\u001b[0;32m    105\u001b[0m \u001b[39m#   Block 1\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m conv1 \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1_1(x))\n\u001b[0;32m    107\u001b[0m conv1 \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1_2(conv1))\n\u001b[0;32m    108\u001b[0m pool1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample1(conv1)\n",
      "File \u001b[1;32mc:\\Users\\jurri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jurri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\jurri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "criterion = torch.nn.L1Loss()\n",
    "MWCNN_trainer = Trainer(MWCNN_model, criterion, device)\n",
    "\n",
    "loss_record = MWCNN_trainer.train(EPOCHS,trainloader,mini_batch=5)\n",
    "\n",
    "print(f'Training finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(MWCNN_model, 'MWCNN_model_L1loss_16.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MWCNN_model = torch.load('MWCNN_model_fft_16.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR: 28.96020724376925, MSE: 2317197.6719563804\n"
     ]
    }
   ],
   "source": [
    "# Testing process on test data.\n",
    "mwcnn_psnr, mwcnn_mse = MWCNN_trainer.test(testloader)\n",
    "\n",
    "print(f'PSNR: {mwcnn_psnr}, MSE: {mwcnn_mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jurri\\Documents\\University\\Life sciences and engineering\\Deep learning in biomedicine\\project\\deep-biomedicine-project\\main.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jurri/Documents/University/Life%20sciences%20and%20engineering/Deep%20learning%20in%20biomedicine/project/deep-biomedicine-project/main.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m image_index \u001b[39m=\u001b[39m test_indices[\u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jurri/Documents/University/Life%20sciences%20and%20engineering/Deep%20learning%20in%20biomedicine/project/deep-biomedicine-project/main.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m sample \u001b[39m=\u001b[39m image_dataset[image_index]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jurri/Documents/University/Life%20sciences%20and%20engineering/Deep%20learning%20in%20biomedicine/project/deep-biomedicine-project/main.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m input_image, pred, output_image, score \u001b[39m=\u001b[39m MWCNN_trainer\u001b[39m.\u001b[39;49mpredict(sample)\n",
      "File \u001b[1;32mc:\\Users\\jurri\\Documents\\University\\Life sciences and engineering\\Deep learning in biomedicine\\project\\deep-biomedicine-project\\src\\training.py:174\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    171\u001b[0m input_image \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39minput_image\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    172\u001b[0m output_image \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39moutput_image\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m--> 174\u001b[0m pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(input_image)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\n\u001b[0;32m    175\u001b[0m pred \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m    177\u001b[0m input_image \u001b[39m=\u001b[39m input_image\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\jurri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jurri\\Documents\\University\\Life sciences and engineering\\Deep learning in biomedicine\\project\\deep-biomedicine-project\\src\\model.py:108\u001b[0m, in \u001b[0;36mMWCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    106\u001b[0m conv1 \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1_1(x))\n\u001b[0;32m    107\u001b[0m conv1 \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1_2(conv1))\n\u001b[1;32m--> 108\u001b[0m pool1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownsample1(conv1)\n\u001b[0;32m    109\u001b[0m \u001b[39m#   Block 2\u001b[39;00m\n\u001b[0;32m    110\u001b[0m conv2 \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2_1(pool1))\n",
      "File \u001b[1;32mc:\\Users\\jurri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jurri\\Documents\\University\\Life sciences and engineering\\Deep learning in biomedicine\\project\\deep-biomedicine-project\\src\\layers.py:12\u001b[0m, in \u001b[0;36mDWT.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 12\u001b[0m     x01 \u001b[39m=\u001b[39m x[:, :, \u001b[39m0\u001b[39;49m::\u001b[39m2\u001b[39;49m, :] \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m     13\u001b[0m     x02 \u001b[39m=\u001b[39m x[: ,:, \u001b[39m1\u001b[39m::\u001b[39m2\u001b[39m, :] \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m     14\u001b[0m     x1 \u001b[39m=\u001b[39m x01[:, :, :, \u001b[39m0\u001b[39m::\u001b[39m2\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 3"
     ]
    }
   ],
   "source": [
    "image_index = test_indices[0]\n",
    "sample = image_dataset[image_index]\n",
    "input_image, pred, output_image, score = MWCNN_trainer.predict(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
